import streamlit as st
import pdfplumber
import os
import re
import io
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple

# Env + Gemini + Chroma
from dotenv import load_dotenv
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# -------------------------
# Streamlit UI Configuration MUST be first
# -------------------------
st.set_page_config(page_title="RAG Sales Analyzer", layout="wide")

# -------------------------
# Load environment variables and Authenticate ðŸ”‘
# -------------------------
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    st.error("âŒ Gemini API key not found in .env file. Please add GEMINI_API_KEY=your_key in .env.")
    # If key is missing, stop execution immediately.
    st.stop()
    
# ðŸš¨ CRITICAL FIX 1: Set the environment variable Chroma requires.
os.environ["CHROMA_GOOGLE_GENAI_API_KEY"] = GEMINI_API_KEY

# Configure Gemini globally for generation calls
genai.configure(api_key=GEMINI_API_KEY) 

# -------------------------
# Constants
# -------------------------
CHROMA_DIR = "chromadb_persist"
COLLECTION_NAME = "sales_pdf_data"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# -------------------------
# Helper functions
# -------------------------
def init_chroma(persist_dir: str = None):
    """Initialize a Chroma client with the stable Gemini embeddings model."""
    client_kwargs = {}
    if persist_dir:
        client_kwargs["persist_directory"] = persist_dir

    # Using the default in-memory client for simplicity
    client = chromadb.Client() 
    
    # Chroma now successfully finds the key set in os.environ in the config block
    embed_fn = embedding_functions.GoogleGenerativeAiEmbeddingFunction(
        model_name="gemini-embedding-001"  
    )

    # Create or reset collection
    try:
        client.delete_collection(COLLECTION_NAME) 
    except Exception:
        pass

    collection = client.create_collection(name=COLLECTION_NAME, embedding_function=embed_fn)
    return client, collection


def chunk_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    chunks = []
    i = 0
    while i < len(text):
        end = i + size
        chunks.append(text[i:end].strip())
        i = end - overlap
        if i < 0:
            i = 0
    return chunks


def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> Tuple[str, List[pd.DataFrame]]:
    """Extract text and tables from PDF."""
    text_parts = []
    tables = []
    try:
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            for page in pdf.pages:
                text = page.extract_text(x_tolerance=2, y_tolerance=2) or ""
                text_parts.append(text)

                try:
                    extracted_tables = page.extract_tables()
                    if extracted_tables:
                        for tbl in extracted_tables:
                            df = pd.DataFrame(tbl)
                            df = df.dropna(axis=0, how="all").dropna(axis=1, how="all")
                            if not df.empty:
                                tables.append(df)
                except Exception:
                    pass
    except Exception as e:
        st.error(f"Failed to process PDF: {e}")
        return "", []
        
    return "\n\n".join(text_parts), tables


def retrieve_relevant_chunks(collection, query: str, n_results: int = 3) -> List[str]:
    try:
        results = collection.query(
            query_texts=[query],
            n_results=n_results,
            include=["documents"]
        )
        docs = results.get("documents", [[]])[0]
        return docs
    except Exception as e:
        return []


def call_gemini_answer(prompt: str, max_output_tokens: int = 1024) -> str:
    """Use Gemini model to generate an answer."""
    try:
        model = genai.GenerativeModel("gemini-2.5-flash") 
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        st.error(f"Gemini call failed: {e}")
        return "Error generating response."


def try_extract_numeric_distribution(text: str) -> Dict[str, float]:
    """
    Extract label-value pairs (like sales by region) from text, handling currency and commas.
    Updated for robust parsing of Zone â€“ Number format and LLM output.
    """
    data = {}
    
    # Strategy 1: Look for "Zone â€“ Number" patterns
    zone_pattern = re.compile(r"^(.+?)\s*â€“\s*[\â‚¹\$]?\s*([0-9,.]+)\s*(?:INR)?(?:\s*\[[0-9.]+%)?\s*$", re.IGNORECASE)
    
    # Strategy 2: Look for lines generated by the LLM that include the value (e.g., "North Zone: 4,10,000")
    llm_breakdown_pattern = re.compile(r"^(.*?Zone):\s*[\â‚¹\$]?\s*([0-9,.]+)\s*(\s*\(.+?\))?(\s*=\s*[0-9.]+%)?$", re.IGNORECASE)

    combined_lines = text.splitlines()

    for line in combined_lines:
        line = line.strip()
        if not line:
            continue
        
        # Check for direct Zone-Number pattern
        m_zone = zone_pattern.match(line)
        if m_zone:
            label = m_zone.group(1).strip()
            num_str = m_zone.group(2).replace(",", "")
            try:
                data[label] = float(num_str)
            except ValueError:
                pass
        
        # Check for LLM breakdown lines
        m_llm = llm_breakdown_pattern.match(line)
        if m_llm:
            label = m_llm.group(1).strip()
            num_str = m_llm.group(2).replace(",", "")
            try:
                data[label] = float(num_str)
            except ValueError:
                pass

    # Filter out irrelevant labels
    filtered_data = {label: value for label, value in data.items() if value > 0 and not any(kw in label.lower() for kw in ["total", "summary", "report period", "prepared by", "executive summary", "calculation"])}

    return filtered_data


def plot_pie_chart(data: Dict[str, float], title: str = "Distribution"):
    labels = list(data.keys())
    sizes = list(data.values())
    
    # Filter out labels with zero or negative size
    filtered_data = {l: s for l, s in data.items() if s > 0}
    if len(filtered_data) < 2:
        st.info("Less than two non-zero data points found to plot the pie chart.")
        return None

    labels = list(filtered_data.keys())
    sizes = list(filtered_data.values())

    # ðŸš¨ CRITICAL FIX 3: Custom color palette using shades of Red/Pink/Burgundy
    custom_red_colors = [
        '#B71C1C',  # Deep Red / Crimson (Darkest)
        '#D32F2F',  # Primary Red
        '#F44336',  # Bright Red
        '#EF9A9A',  # Light Red / Pale Pink
        '#FFCDD2',  # Very Light Pink
        '#880E4F',  # Deep Burgundy / Maroon (Alternative Dark)
        '#E91E63',  # Vibrant Pink / Magenta
        '#F06292',  # Medium Pink
    ]
    
    # Ensure we have enough colors for all slices
    colors_to_use = (custom_red_colors * ((len(labels) // len(custom_red_colors)) + 1))[:len(labels)]

    
    fig, ax = plt.subplots(figsize=(4, 4))
    ax.pie(sizes, labels=labels, autopct="%1.1f%%", startangle=90, 
           wedgeprops={'linewidth': 0.5, 'edgecolor': 'white'},
           colors=colors_to_use) # Pass the custom red colors here!
    ax.axis("equal")
    ax.set_title(title, fontsize=12)
    return fig

# -------------------------
# Streamlit UI
# -------------------------

st.title("ðŸ“Š Sales Data Analyzer")
st.caption("Upload a sales PDF, embed the data, ask questions, and get answers with pie chart visualization.")

# Initialize Chroma and store in session state
if 'chroma_init' not in st.session_state:
    client, collection = init_chroma(persist_dir=CHROMA_DIR)
    st.session_state.client = client
    st.session_state.collection = collection
    st.session_state.chroma_init = True
else:
    client = st.session_state.client
    collection = st.session_state.collection


# Sidebar settings
with st.sidebar:
    st.header("âš™ï¸ Options")
    n_results = st.number_input("Top results (k)", min_value=1, max_value=10, value=3)
    st.markdown(f"**Chunk Size:** {CHUNK_SIZE}, **Overlap:** {CHUNK_OVERLAP}")


uploaded_file = st.file_uploader("ðŸ“„ Upload your sales report (PDF)", type=["pdf"])

if uploaded_file:
    bytes_data = uploaded_file.read()
    
    # Use session state to process PDF only once per upload
    if 'pdf_processed_hash' not in st.session_state or st.session_state.pdf_processed_hash != uploaded_file.file_id:
        with st.spinner("Extracting text and tables..."):
            text, extracted_tables = extract_text_from_pdf_bytes(bytes_data)
            st.session_state.text = text
            st.session_state.extracted_tables = extracted_tables
            st.session_state.pdf_processed_hash = uploaded_file.file_id
            st.session_state.chunks = chunk_text(text, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)

    st.success(f"âœ… Extracted text length: {len(st.session_state.text)} characters")

    if st.session_state.extracted_tables:
        st.write("Detected tables (preview):")
        for df in st.session_state.extracted_tables[:2]:
            st.dataframe(df.head())

    st.write(f"ðŸ“‘ Ready to embed {len(st.session_state.chunks)} chunks")

    # Use a specific session state key for the uploaded file's embedding status
    embed_key = f"embedded_{uploaded_file.file_id}"

    if st.button("Embed & store in ChromaDB (Reset)"):
        with st.spinner("Embedding and storing..."):
            # Re-initialize collection to clear old data (this uses the fixed init_chroma)
            st.session_state.client, st.session_state.collection = init_chroma(persist_dir=CHROMA_DIR)
            
            ids = [f"doc_{i}" for i in range(len(st.session_state.chunks))]
            metadatas = [{"chunk_index": i} for i in range(len(st.session_state.chunks))]
            
            st.session_state.collection.add(documents=st.session_state.chunks, ids=ids, metadatas=metadatas)
            st.session_state[embed_key] = True
            st.success("âœ… Data stored successfully! You can now ask questions.")

    if st.session_state.get(embed_key, False):
        st.divider()
        st.markdown("### ðŸ’¬ Ask a question")

        # Use st.form to allow the Enter key to submit and trigger the RAG logic
        with st.form(key='rag_query_form'):
            query = st.text_input("Enter your question about the report:", key="query_input")
            submit_button = st.form_submit_button("Get Answer")

        if submit_button and query:
            
            # --- RAG Retrieval Step ---
            with st.spinner("Retrieving relevant chunks..."):
                retrieved = retrieve_relevant_chunks(st.session_state.collection, query, n_results)
            
            if not retrieved:
                st.error("âŒ Failed to retrieve context. Please check API key, re-embed data, and ensure your question is relevant.")
                st.stop()
            
            st.write("#### Retrieved Context")
            for i, r in enumerate(retrieved):
                st.markdown(f"**Chunk {i+1}:**")
                st.code(r[:500] + ("..." if len(r) > 500 else ""), language='text')

            context = "\n\n---\n\n".join(retrieved)
            prompt = f"""You are a data analysis assistant. 
Use the context below to answer the user's question clearly and concisely.
If the question asks for a total of several items (like zones), provide the calculation.

Context:
{context}

Question:
{query}

Answer in plain language. If relevant, provide numeric insights or percentages."""

            # --- Gemini Generation Step ---
            with st.spinner("Getting your answer..."):
                answer = call_gemini_answer(prompt)

            st.subheader("ðŸ§  Your Answer is here")
            st.write(answer)

            # --- Chart Generation Step ---
            if any(x in query.lower() for x in ["pie", "distribution", "share", "percent", "sales by", "breakdown", "regional", "q3 sales"]):
                st.subheader("ðŸ“Š Extracted Pie Chart")
                
                combined_text = context + "\n\n" + answer
                for df in st.session_state.extracted_tables:
                    try:
                        combined_text += "\n\nTable Data:\n" + df.to_csv(index=False, sep='\t')
                    except:
                        pass

                distribution = try_extract_numeric_distribution(combined_text)
                
                if distribution:
                    fig = plot_pie_chart(distribution, "Extracted Distribution")
                    if fig:
                        st.pyplot(fig)
                        st.dataframe(pd.DataFrame(list(distribution.items()), columns=["Label", "Value"]))
                else:
                    st.info("No clear numeric distribution found to plot.")